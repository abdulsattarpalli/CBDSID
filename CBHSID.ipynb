{
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "class ClusterBasedOverUnderSampling:\n\n    def check_data_type(self, var):\n        if isinstance(var, pd.Series) or isinstance(var, pd.DataFrame):\n            var = var.values\n        return var\n\n    def __init__(self, X_test, y_test):\n        self.X_test = self.check_data_type(X_test)\n        self.y_test = self.check_data_type(y_test)\n\n        class_labels = np.unique(y_test)\n        self.avg = len(y_test) // len(class_labels)\n        print(\"Average Size: {}\".format(self.avg))\n\n    def under_sample_data(self, MJ_class, MJ_class_data):\n        items_after_us = list()\n        # removed_indexes = list()\n\n        MJ_class_size = MJ_class_data.shape[0]\n\n        #clustering = AffinityPropagation(random_state=0,convergence_iter=20,max_iter=250,preference=-50)\n        clustering = AffinityPropagation()\n        clustering.fit(MJ_class_data)\n        sub_clusters = clustering.labels_        \n        centers_indices = clustering.cluster_centers_indices_\n        print('centers_indices: {}',format(centers_indices))\n        print('MJ_class_size :',MJ_class_size)\n        labels = clustering.labels_                \n        cluster_centers_indices = MJ_class_data.iloc[centers_indices, :].index\n        print('Here is the exception Booooommmmmmm')\n        sub_clusters, sub_cluster_element_count = np.unique(sub_clusters, return_counts=True)\n        NSTR_from_MJ_class = 0\n    #     print(\"-\" * 80)\n    #     print(\"Class: {}, size: {}\".format(MJ_class, MJ_class_size))\n        for sub_cluster, sub_cluster_size in zip(sub_clusters, sub_cluster_element_count):\n            NSTR = sub_cluster_size - round((sub_cluster_size * self.avg) / MJ_class_size)\n            NSTR_from_MJ_class = NSTR_from_MJ_class + NSTR\n            \n            #print(\"sub_cluster:{},\\tsub_cluster_size:{},\\tremove:{}\".format(sub_cluster, sub_cluster_size, NSTR))\n\n            #sub_cluster_members = labels == sub_cluster\n            #sub_cluster_elements = MJ_class_data[sub_cluster_members]\n            #sub_cluster_elements_index_list = sub_cluster_elements.index\n            sub_cluster_elements_mean = np.mean(MJ_class_data[labels == sub_cluster], axis=1)\n            sub_cluster_center_index = cluster_centers_indices[sub_cluster]\n    #         break\n            distances = dict()\n            # cc = 0\n            for key in sub_cluster_elements_mean.keys():\n                distance = abs(sub_cluster_elements_mean[sub_cluster_center_index] - sub_cluster_elements_mean[key])\n                distances[key] = dict()\n                distances[key] = distance\n\n            N = int(sub_cluster_size - NSTR)\n            items_closed_to_centroid = nsmallest(N, distances, key = distances.get)\n            items_after_us.extend(items_closed_to_centroid)\n            #print(\"items_closed_to_centroid: {}\\n\".format(items_closed_to_centroid))\n\n    #         all_indexes = sub_cluster_elements_index_list.to_list()\n    #         removed_index = list(set(all_indexes) - set(items_closed_to_centroid))\n    #         removed_indexes.extend(removed_index)\n\n            #print(\"\\nClass: {}, size:{}, Remove:{}, After US:{}\\n\".format(MJ_class, MJ_class_size,NSTR_from_MJ_class, (MJ_class_size-NSTR_from_MJ_class)))\n    #     print('+' * 10)\n        print(\"After US size: {}\\n\".format(items_after_us))\n\n        features = self.X_test[items_after_us, :]\n        #print(\"features: {}\\n\".format(len(features)))\n        labels = self.y_test[items_after_us]      \n        \n        #print(\"labels: {}\\n\".format(labels))\n        all_data = np.append(features, labels.reshape(-1, 1), axis=1)\n        df_sample = pd.DataFrame(all_data, index=items_after_us)\n        df_sample.iloc[:, -1] = df_sample.iloc[:, -1].astype(int)\n        #print(\"all_data: {}\\n\".format(all_data))\n\n        return df_sample\n\n    def over_sample_data(self, MI_class, MI_class_data):\n\n        # MI_class_data = minority[MI_class]['values']\n        MI_class_size = MI_class_data.shape[0]\n\n        #clustering = AffinityPropagation(convergence_iter=20)\n        clustering = AffinityPropagation()\n        clustering.fit(MI_class_data)\n        sub_clusters = clustering.labels_\n\n        centers_indices = clustering.cluster_centers_indices_\n        labels = clustering.labels_\n\n        cluster_centers_indices = MI_class_data.iloc[centers_indices, :].index\n\n        sub_clusters, sub_cluster_element_count = np.unique(sub_clusters, return_counts=True)\n        NSTA_into_MI_class = 0\n        items_after_os = list()\n        generated = list()\n    #     print(\"Class: {}, size: {}\".format(MI_class, MI_class_size))\n        for sub_cluster, sub_cluster_size in zip(sub_clusters, sub_cluster_element_count):\n            NSTA = round((sub_cluster_size * self.avg) / MI_class_size) - sub_cluster_size\n            NSTA_into_MI_class = NSTA_into_MI_class + NSTA\n\n            # N_STA = int(NSTA)\n    #         print(\"sub_cluster:{},\\tsub_cluster_size:{},\\tadd:{}\".format(sub_cluster, sub_cluster_size, N_STA))\n\n            # When NSTA Value is zero then do not adding item to cluster\n    #         if not bool(NSTA):\n    #             continue\n\n            sub_cluster_members = labels == sub_cluster\n            sub_cluster_elements = MI_class_data[sub_cluster_members]\n            sub_cluster_elements_index_list = sub_cluster_elements.index\n            sub_cluster_elements_mean = np.mean(MI_class_data[labels == sub_cluster], axis=1)\n            sub_cluster_center_index = cluster_centers_indices[sub_cluster]\n            sub_cluster_center_mean = sub_cluster_elements_mean[sub_cluster_center_index]\n\n            # here need to add NSTA new sample, so generate NSTA sample that are very similar to the existing samples in the\n            # current cluster\n\n            #N = int(sub_cluster_size + NSTA)\n            # approach would be select NSTA sample which are away from the center and than generate NSTA sample similare to\n            # those\n\n            N = int(NSTA)\n            distances = dict()\n            for key in sub_cluster_elements_mean.keys():\n                distance = abs(sub_cluster_elements_mean[sub_cluster_center_index] - sub_cluster_elements_mean[key])\n                distances[key] = dict()\n                distances[key] = distance\n\n\n            #items_away_from_centroid = nlargest(N, distances, key = distances.get)\n            items_away_from_centroid = nsmallest(N, distances, key = distances.get) # lets try nearest sample for oversamping 16-06-21\n            \n            item = 0\n            for item in range(len(items_away_from_centroid)):\n                items_after_os.append(items_away_from_centroid[item])\n\n    #         print(\"Means of selected rows: {}\".format(sub_cluster_elements_mean))\n    #         print(\"Items selected for OS: \",MI_class_data[labels == sub_cluster])\n    #         print(\"Indexes of selected rows: {}\".format(items_after_os))\n\n    #         synthetic_item = Xi + (Xi - Xj) * sub_cluster_center_mean\n    #         Xi individual item of minority class in the selected items\n    #         Xj individual any item of minority class in the selected items\n\n    #         sort_distances = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n    #         sort_distances\n    #         print(\"difference: \",difference.item())\n\n    #         res = dict(sorted(difference.items(), key = itemgetter(1), reverse = True)[:N])\n    #         print(\"Top {} itmes: {}\".format(NSTR,remaining_items))\n    #         print(\"mean at center is {}  minus item mean {} = {}\".format(sub_cluster_elements_mean[sub_cluster_center_index],sub_cluster_elements_mean[key],difference))\n    #         break\n\n            element_away = items_away_from_centroid.copy() # This is list\n            N_samples_to_add = int(NSTA)\n    #         generated = list()\n            # num_new_rows = len(items_away_from_centroid)\n            new_count = sub_cluster_size + N_samples_to_add\n    #         print(\"\\t\\tAdding {} new rows in existing {} rows = {}...\".format(N_samples_to_add, sub_cluster_size, new_count))\n            for x in range(new_count):\n                num_rows = x + 1\n\n                normal_flow = True\n                if num_rows <= N_samples_to_add and bool(element_away):\n                    an_index = element_away.pop()\n    #                 print(num_rows, \": Picked index {} for data normally\".format(an_index))\n                elif N_samples_to_add > sub_cluster_size:\n                    normal_flow = False\n                    an_index = random.choice(items_away_from_centroid)\n    #                 print(num_rows, \": Random index {} is picked\".format(an_index))\n                else:\n    #                 print(\"Do Nothing\")\n                    normal_flow = None\n\n                if normal_flow is None:\n                    curr_elements = sub_cluster_elements.values.tolist()\n                    generated.extend(curr_elements)\n                    break\n\n                row_values = sub_cluster_elements[sub_cluster_elements_index_list == an_index].values.flatten()\n                last_col_num = row_values.shape[0] - 1\n                cluster_center_last_col_val = sub_cluster_elements.iloc[sub_cluster_elements_index_list == sub_cluster_center_index, last_col_num].values[0]\n\n                generate_values = list()\n                for col_num, val in enumerate(row_values):\n                    xi = val\n                    if col_num == last_col_num:\n                        xj = cluster_center_last_col_val\n                    else:\n                        next_column_index = col_num + 1\n                        if normal_flow:\n                            xj = row_values[next_column_index]\n                        else:\n                            index_other = random.choice(items_away_from_centroid)\n                            other = sub_cluster_elements.iloc[sub_cluster_elements_index_list == index_other, next_column_index]\n                            xj = other.values.flatten()[0]\n\n                    generate_value = random.uniform(0.001, 0.005) + xi # Thia formula preformed better than the orignal formula \n                    #generate_value = xi + (abs(xi - xj)) * sub_cluster_center_mean # original formula\n                    #generate_value =  (abs(xi - xj)) * sub_cluster_center_mean # changed for improving acc for class 0 : 15 june 21\n                    generate_values.append(generate_value)\n                generated.append(generate_values)\n\n        generated_data = np.array(generated)\n        synthetic_data = pd.DataFrame(generated_data)\n\n        last_column = synthetic_data.columns.shape[0]\n        synthetic_data[last_column] = MI_class\n\n        return synthetic_data\n\n    def fit(self):\n\n        true_labels = np.unique(self.y_test)\n        # df_list = dict()\n        majority = dict()\n        minority = dict()\n\n        all_data = list()\n        # all_balanced_data = list()\n        for cluster_number in true_labels:\n            class_size = np.count_nonzero(self.y_test == cluster_number)\n            # sample_diff = class_size - self.avg\n            row_ix = np.where(self.y_test == cluster_number)\n            indexes = row_ix[0]\n            data = pd.DataFrame(self.X_test[indexes, :], index=indexes)\n\n            if class_size > self.avg:\n                majority[cluster_number] = dict()\n                majority[cluster_number]['values'] = data\n                print(\"{}: {} - major\".format(cluster_number, data.shape[0]))\n\n                # Perform Under Sampling of Data\n                df_data = self.under_sample_data(cluster_number, data)\n                print(\"After Under Sample Data For Class # {}: {}\".format(cluster_number, df_data.shape))\n                all_data.append(df_data)\n            else:\n                minority[cluster_number] = dict()\n                minority[cluster_number]['values'] = data\n                print(\"{}: {} - minor\".format(cluster_number, data.shape[0]))\n\n                # Perform Over Sampling of Data\n                df_data = self.over_sample_data(cluster_number, data)\n                print(\"After Over Sampled Data For Class # {}: {}\".format(cluster_number, df_data.shape))\n                all_data.append(df_data)\n\n        balanced_data = pd.concat(all_data, axis=0, ignore_index=True)\n        x = balanced_data.iloc[:, :-1]\n        y = balanced_data.iloc[:, -1]\n\n        return x, y",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}